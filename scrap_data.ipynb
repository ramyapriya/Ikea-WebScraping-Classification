{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scraping from Ikea Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide website link and name of the class for which images needs to be downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "website = 'https://www.ikea.com/in/en/cat/beds-bm003/'\n",
    "data_dir = 'beds'\n",
    "page = requests.get(website)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "soup =  soup.find('div', attrs={'class': 'catalog-product-list'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all links of the subcategories\n",
    "all_items_outer_div = soup.find('div', attrs={'class' : 'plp-revamp-product-list__product'} )\n",
    "inner_items_data = all_items_outer_div.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Double beds https://www.ikea.com/in/en/cat/double-beds-16284/\n",
      "Guest beds & day beds https://www.ikea.com/in/en/cat/daybeds-19046/\n",
      "Single beds https://www.ikea.com/in/en/cat/single-beds-16285/\n",
      "Loft beds & bunk beds loft-beds-bunk-beds\n",
      "Sofa beds & chair beds https://www.ikea.com/in/en/cat/sofa-beds-10663/\n",
      "Children's beds https://www.ikea.com/in/en/cat/children-s-beds-18723/\n",
      "Children's beds 8-12 https://www.ikea.com/in/en/cat/children-s-beds-8-12-24708/\n"
     ]
    }
   ],
   "source": [
    "# save product name and it's link in a list \n",
    "product_data = []\n",
    "for item in inner_items_data:\n",
    "    product_name = item.text.strip()\n",
    "    product_link = item.attrs['href']\n",
    "    print(product_name,product_link)\n",
    "    product_data.append([product_name, product_link])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Download images and save them into their respective directory\n",
    "    Input: List containing names of subcategory and their page link\n",
    "    Output: List containing names and link of subcategory which failed to download\n",
    "'''\n",
    "def download_images(product_data):\n",
    "    skipped_data = []\n",
    "    for product_name, product_link in product_data:\n",
    "        product_dir = os.path.join(data_dir,product_name)\n",
    "        if not os.path.exists(product_dir):\n",
    "            os.makedirs(product_dir)\n",
    "        next_page_available = True\n",
    "        counter = 0\n",
    "        pages_to_scrap = 3\n",
    "        while pages_to_scrap:\n",
    "            try:\n",
    "                product_page = requests.get(product_link)\n",
    "                soup = BeautifulSoup(product_page.text, 'html.parser')\n",
    "                next_page_link = soup.find('a', {'class' : 'pagination__right button button--primary' })\n",
    "                soup =  soup.find(\"div\", {\"class\": \"range-product-list__products\"})\n",
    "                images_data = soup.findAll('img')\n",
    "                for image_data in images_data:\n",
    "                    img_link = image_data.attrs['src'][:image_data.attrs['src'].rfind('?')] + '?f=xxxs'\n",
    "                    urllib.request.urlretrieve(img_link, product_dir + '/' + str(counter) + '.jpg')\n",
    "                    counter += 1\n",
    "                if next_page_link is None:\n",
    "                    break\n",
    "                product_link = next_page_link.attrs['href']\n",
    "                pages_to_scrap -= 1\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                skipped_data.append( [product_name,product_link] )\n",
    "                break\n",
    "        print('Total {} {} images downloaded'.format(counter,product_name))\n",
    "    return skipped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 24 Double beds images downloaded\n",
      "Total 15 Guest beds & day beds images downloaded\n",
      "Total 26 Single beds images downloaded\n",
      "Invalid URL 'loft-beds-bunk-beds': No schema supplied. Perhaps you meant http://loft-beds-bunk-beds?\n",
      "Total 0 Loft beds & bunk beds images downloaded\n",
      "Total 64 Sofa beds & chair beds images downloaded\n",
      "Total 40 Children's beds images downloaded\n",
      "'NoneType' object has no attribute 'findAll'\n",
      "Total 0 Children's beds 8-12 images downloaded\n"
     ]
    }
   ],
   "source": [
    "skipped_data = download_images(product_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bunkbeds https://www.ikea.com/in/en/cat/bunkbeds-19048/\n",
      "Loft beds https://www.ikea.com/in/en/cat/loft-beds-19049/\n",
      "Total 3 Bunkbeds images downloaded\n",
      "Total 8 Loft beds images downloaded\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "check for skipped data. Sometimes our selected category contains a category which has sub categories \n",
    "inside it. i.e. it has separate page for it.\n",
    "'''\n",
    "skipped = None\n",
    "for product_name, product_link in skipped_data:\n",
    "    if 'www.ikea.com' not in product_link:\n",
    "        product_page = requests.get('https://www.ikea.in/' + product_link)\n",
    "        soup = BeautifulSoup(product_page.text, 'html.parser')\n",
    "        soup =  soup.find(\"div\", {\"class\": \"product_cat_gallery\"})\n",
    "        all_items_outer_div = soup.find(\"div\", {\"class\" : \"row justify-content-center\"} )\n",
    "        inner_items_data = all_items_outer_div.find_all('a')\n",
    "        product_data = []\n",
    "        for item in inner_items_data:\n",
    "            name = item.text.strip()\n",
    "            link = item.attrs['href']\n",
    "            print(name,link)\n",
    "            product_data.append([name, link])\n",
    "        skipped = download_images(product_data)\n",
    "print(skipped)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
